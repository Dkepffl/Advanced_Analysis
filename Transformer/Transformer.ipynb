{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qhsbaa7nn0Vu",
        "DjLRqsadRSVv",
        "osTdOJueRNmo",
        "1c9YUo0mn-t2",
        "IeXo-hitgJFk",
        "sJBlibJSoGRH",
        "wkhuFLZkoTeV",
        "lOo3enKnofCX",
        "3z1VLYEt5pXF",
        "CAL6MuaJor6Y",
        "827gYspjo1zW",
        "Vwa9EGy-mqyG"
      ],
      "authorship_tag": "ABX9TyN5Nz7yXJqHGZbj9OavlLyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dkepffl/Advanced_Analysis/blob/main/Transformer/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Packages**"
      ],
      "metadata": {
        "id": "8w_8koWBa6G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-OTuX_9EbES5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data loader**"
      ],
      "metadata": {
        "id": "SJJSOhykbFbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "path = './datasets/'\n",
        "\n",
        "# Data transform Setting\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # Tensor 변환. 다른 전처리X\n",
        "\n",
        "# Load data\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=0)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=0)"
      ],
      "metadata": {
        "id": "s_iY1Eo3bNlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c14204-af15-40a3-8968-65a49441dbeb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 77.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_data[0][0].shape # 출력 결과 : [3, 32, 32]\n",
        "output_shape = len(train_data.classes) # 출력 결과 : 100"
      ],
      "metadata": {
        "id": "XWjsTu3dlKcI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer 구현**"
      ],
      "metadata": {
        "id": "QM2IkmKdlYfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "#### there is nothing to do upto here ####\n",
        "##########################################"
      ],
      "metadata": {
        "id": "i41zWBTWbOie"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positional Encoding**\n",
        "```python\n",
        "# argument 정리\n",
        "pe = PositionalEncoding(device, max_len, d_model)\n",
        "pos_emb = pe(x)\n",
        "```"
      ],
      "metadata": {
        "id": "qhsbaa7nn0Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.5 in the paper\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, device, max_len=512, d_model=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # max_len만큼 만들고 x의 seq_len만큼 잘라서 사용\n",
        "        self.pos_enc = torch.zeros(self.max_len, self.d_model,requires_grad=False, device=self.device)\n",
        "        # or self.pos_enc.to(self.device)\n",
        "\n",
        "        # Position Encoding Matrix\n",
        "        pos = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(-1,) # unsqueeze :->(max_len,1)\n",
        "        ii = torch.arange(0, self.d_model, step=2, dtype=torch.float) # 2i. (1, self.d_model)\n",
        "\n",
        "        '''\n",
        "        논문을 그대로 따르려면\n",
        "        i = torch.arange(0, self.d_model//2, step=1, dtype=torch.float)\n",
        "\n",
        "        self.pos_enc[:,0::2] = torch.sin(pos/(10000**((2*i)/self.d_model)))\n",
        "        self.pos_enc[:,1::2] = torch.cos(pos/(10000**((2*i)/self.d_model)))\n",
        "\n",
        "        torch.arange() 메서드 안에서 self.d_model//2 계산하는 것이 싫어서 아예 2i를 generate\n",
        "        -> Q. O(n)을 고려할 때 좋은 방법은?\n",
        "        '''\n",
        "        self.pos_enc[:,0::2] = torch.sin(pos/(10000**(ii/self.d_model)))\n",
        "        self.pos_enc[:,1::2] = torch.cos(pos/(10000**(ii/self.d_model)))\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        x: transformed input embedding where x.shape = [batch_size, seq_len, data_dim]\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        pos_emb = self.pos_enc[:seq_len,:]\n",
        "\n",
        "        return pos_emb # input embedding + pos_emb는 어디서 해야 하지?"
      ],
      "metadata": {
        "id": "hH8H_dWbn8lm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **idea**"
      ],
      "metadata": {
        "id": "DjLRqsadRSVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To make Position Encoding Matrix\n",
        "temp = torch.zeros(4,4,requires_grad=False)\n",
        "\n",
        "temp2 = torch.arange(4).unsqueeze(-1,) # row == pos\n",
        "temp3 = torch.arange(0, 4//2, 1) # column == i\n",
        "\n",
        "temp[:,0::2] = torch.sin(temp2/(10000**((2*temp3)/4)))\n",
        "temp[:,1::2] = torch.cos(temp2/(10000**((2*temp3)/4)))\n",
        "\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyxUQ5xnMWux",
        "outputId": "8aee741f-c60b-450a-e213-e5f3951cc0b5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
              "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
              "        [ 0.1411, -0.9900,  0.0300,  0.9996]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2\n",
        "temp = torch.zeros(4,4,requires_grad=False)\n",
        "\n",
        "temp2 = torch.arange(4).unsqueeze(-1,) # row == pos\n",
        "temp3 = torch.arange(0, 4, 2) # column == 2*i\n",
        "\n",
        "temp[:,0::2] = torch.sin(temp2/(10000**((temp3)/4)))\n",
        "temp[:,1::2] = torch.cos(temp2/(10000**((temp3)/4)))\n",
        "\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWEfsHiFVUCf",
        "outputId": "bd3490d2-a1ea-4ed8-b535-e4e916d287bd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
              "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
              "        [ 0.1411, -0.9900,  0.0300,  0.9996]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 코드 결과 동일"
      ],
      "metadata": {
        "id": "2PddcDmsYvRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reference**"
      ],
      "metadata": {
        "id": "osTdOJueRNmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 논문에서 참고하라고 한 논문\n",
        "  - [Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu\n",
        "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.](https://arxiv.org/pdf/1705.03122)"
      ],
      "metadata": {
        "id": "haWPDajHWM5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ScaledDotProductAttention**\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt {d_{k}}})V$$\n",
        "```python\n",
        "attention = ScaledDotProductAttention()\n",
        "attention_value = attention(q, k, v, mask=None)\n",
        "```"
      ],
      "metadata": {
        "id": "1c9YUo0mn-t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.2.1 and Fig 2 (left) in the paper\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "        # compute attention value based on transformed query, key, value where mask is given conditionally\n",
        "        \"\"\"\n",
        "        q, k, v = transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, num_head, seq_len, d=d_model/num_head]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        k_t =  torch.transpose(k, -1, -2) # [batch_size, num_head, d, seq_len]\n",
        "\n",
        "        numerator = torch.matmul(q, k_t)\n",
        "        denominator = torch.sqrt(q.shape[-1]) # d = q.shape[-1]\n",
        "\n",
        "        attention_value = numerator/denominator\n",
        "\n",
        "        if mask != None:\n",
        "          # if the index has value False, kill the value; else, leave the value\n",
        "          # 논문에서는 -inf\n",
        "          attention_value = torch.mul(mask, attention_value)\n",
        "\n",
        "        attention_value = self.softmax(attention_value)\n",
        "        attention_value = torch.matmul(attention_value,v)\n",
        "\n",
        "        return attention_value"
      ],
      "metadata": {
        "id": "Onq2dngLoD-u"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **idea**"
      ],
      "metadata": {
        "id": "IeXo-hitgJFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mask=torch.tensor([[True, False, True], [False, True, False]], dtype=torch.bool)\n",
        "temp_x = torch.tensor([[1,2,3],[4,5,6]], dtype = torch.int)\n",
        "\n",
        "temp_ans = torch.tensor([[1,0, 3],[0, 5, 0]], dtype=torch.int)\n",
        "temp_ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEiAImJ2fN7s",
        "outputId": "c08e165a-8661-469f-9a0f-d7cae636ae5c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 3],\n",
              "        [0, 5, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_x[temp_mask]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKril4xLfef5",
        "outputId": "dea1a3b8-2247-417c-e4cc-b0b1a4bb7ed6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 3, 5], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mul(temp_mask, temp_x) # elementwise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsYzxlBFhRla",
        "outputId": "0772ecce-42dc-4c9d-9d93-6bedc07f9e8e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 3],\n",
              "        [0, 5, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi Head Attention(수정 필요)**\n",
        "$$MultiHead(Q, K, V) = Concat(head_1, ... , head_h)W^{O}$$\n",
        "\n",
        "where $  head_i = Attention(QW^{Q}_{i}, QW^{K}_{i}, QW^{V}_{i})$\n",
        "\n",
        "```python\n",
        "multiheadattention = MultiHeadAttention(d_model, num_head)\n",
        "output = multiheadattention(q, k, v, mask=None)\n",
        "```"
      ],
      "metadata": {
        "id": "sJBlibJSoGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.2.2 and Fig 2 (right) in the paper\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,d_model=16,num_head=4):\n",
        "        super().__init__()\n",
        "        # fill out the rest\n",
        "        assert d_model % num_head == 0, \"check if d_model is divisible by num_head\"\n",
        "\n",
        "        # dimension\n",
        "        self.d_model = d_model\n",
        "        self.num_head = num_head # head 개수 : h\n",
        "        self.d = d_model//num_head # d_k = d_v = d_model/h\n",
        "\n",
        "        # module\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        # W_i : learnable\n",
        "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_k = nn.Linear(self.d_model, self.d_model)\n",
        "        self.w_v = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.w_o = nn.Linear(self.d*self.num_head, self.d_model)\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "        # fill out here\n",
        "        # compute multi-head attention value\n",
        "        # here, query, key, value are pre-transformed, so you need to transfrom them in this module\n",
        "        \"\"\"\n",
        "        q, k, v = pre-transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, seq_len, d_model]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        batch_size = q.shape[0]\n",
        "        seq_len = q.shape[1]\n",
        "\n",
        "        QW = self.w_q(q) # [batch_size, seq_len, d_model]\n",
        "        KW = self.w_k(k)\n",
        "        VW = self.w_v(v)\n",
        "        '''\n",
        "        Note that\n",
        "\n",
        "        ScaledDotProductAttention의 input shape : [batch_size, num_head, seq_len, d]\n",
        "\n",
        "        현재 : [batch_size, seq_len, d_model]\n",
        "        reshape -> [batch_size, seq_len, num_head, d]\n",
        "        transpose(1,2) -> [batch_size, num_head, seq_len, d]\n",
        "\n",
        "        Colab에서 view()도 비슷한 기능이라고 추천하는데 안 써봐서 reshape 씀.\n",
        "        '''\n",
        "        QW = QW.reshape((batch_size, seq_len, self.num_head, self.d)).transpose(1,2)\n",
        "        KW = KW.reshape((batch_size, seq_len, self.num_head, self.d)).transpose(1,2)\n",
        "        VW = VW.reshape((batch_size, seq_len, self.num_head, self.d)).transpose(1,2)\n",
        "\n",
        "        head = self.attention(QW, KW, VW, mask)\n",
        "        con_head = head.transpose(1,2).contiguous().view(batch_size, -1, self.d_model) # -> [batch_size, -1 ,d_model]\n",
        "\n",
        "        output = self.w_o(con_head)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "-JD0KXDtoO7M"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Idea**"
      ],
      "metadata": {
        "id": "hdljDJlGp0kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp=torch.arange(0, 120).reshape((4,5,6))\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijc9zpmBn15N",
        "outputId": "76c3439f-0b7c-405b-fe09-cb4edd2a0204"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  0,   1,   2,   3,   4,   5],\n",
              "         [  6,   7,   8,   9,  10,  11],\n",
              "         [ 12,  13,  14,  15,  16,  17],\n",
              "         [ 18,  19,  20,  21,  22,  23],\n",
              "         [ 24,  25,  26,  27,  28,  29]],\n",
              "\n",
              "        [[ 30,  31,  32,  33,  34,  35],\n",
              "         [ 36,  37,  38,  39,  40,  41],\n",
              "         [ 42,  43,  44,  45,  46,  47],\n",
              "         [ 48,  49,  50,  51,  52,  53],\n",
              "         [ 54,  55,  56,  57,  58,  59]],\n",
              "\n",
              "        [[ 60,  61,  62,  63,  64,  65],\n",
              "         [ 66,  67,  68,  69,  70,  71],\n",
              "         [ 72,  73,  74,  75,  76,  77],\n",
              "         [ 78,  79,  80,  81,  82,  83],\n",
              "         [ 84,  85,  86,  87,  88,  89]],\n",
              "\n",
              "        [[ 90,  91,  92,  93,  94,  95],\n",
              "         [ 96,  97,  98,  99, 100, 101],\n",
              "         [102, 103, 104, 105, 106, 107],\n",
              "         [108, 109, 110, 111, 112, 113],\n",
              "         [114, 115, 116, 117, 118, 119]]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.reshape((4,-1,2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrNFVUZFplaz",
        "outputId": "93336845-c9d9-4a34-8650-2e2929bb65bc"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[  0,   1,   2],\n",
              "          [  3,   4,   5]],\n",
              "\n",
              "         [[  6,   7,   8],\n",
              "          [  9,  10,  11]],\n",
              "\n",
              "         [[ 12,  13,  14],\n",
              "          [ 15,  16,  17]],\n",
              "\n",
              "         [[ 18,  19,  20],\n",
              "          [ 21,  22,  23]],\n",
              "\n",
              "         [[ 24,  25,  26],\n",
              "          [ 27,  28,  29]]],\n",
              "\n",
              "\n",
              "        [[[ 30,  31,  32],\n",
              "          [ 33,  34,  35]],\n",
              "\n",
              "         [[ 36,  37,  38],\n",
              "          [ 39,  40,  41]],\n",
              "\n",
              "         [[ 42,  43,  44],\n",
              "          [ 45,  46,  47]],\n",
              "\n",
              "         [[ 48,  49,  50],\n",
              "          [ 51,  52,  53]],\n",
              "\n",
              "         [[ 54,  55,  56],\n",
              "          [ 57,  58,  59]]],\n",
              "\n",
              "\n",
              "        [[[ 60,  61,  62],\n",
              "          [ 63,  64,  65]],\n",
              "\n",
              "         [[ 66,  67,  68],\n",
              "          [ 69,  70,  71]],\n",
              "\n",
              "         [[ 72,  73,  74],\n",
              "          [ 75,  76,  77]],\n",
              "\n",
              "         [[ 78,  79,  80],\n",
              "          [ 81,  82,  83]],\n",
              "\n",
              "         [[ 84,  85,  86],\n",
              "          [ 87,  88,  89]]],\n",
              "\n",
              "\n",
              "        [[[ 90,  91,  92],\n",
              "          [ 93,  94,  95]],\n",
              "\n",
              "         [[ 96,  97,  98],\n",
              "          [ 99, 100, 101]],\n",
              "\n",
              "         [[102, 103, 104],\n",
              "          [105, 106, 107]],\n",
              "\n",
              "         [[108, 109, 110],\n",
              "          [111, 112, 113]],\n",
              "\n",
              "         [[114, 115, 116],\n",
              "          [117, 118, 119]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PositionwiseFeedForwardNetwork**\n",
        "```python\n",
        "ffn = PositionwiseFeedForwardNetwork(d_model, num_head)\n",
        "output = ffn(x)\n",
        "```"
      ],
      "metadata": {
        "id": "wkhuFLZkoTeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.3 in the paper\n",
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self,d_model=16,d_ff=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.linear1 = nn.Linear(self.d_model, self.d_ff) # W1\n",
        "        self.linear2 = nn.Linear(self.d_ff, self.d_model) # W2\n",
        "        self.relu = nn.ReLU()  # max(0, xW1 + b1) : ReLU\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.linear1(x)\n",
        "        output = self.relu(output)\n",
        "        output = self.linear2(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "suaqHOhGoXcA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Masking(수정 필요)**"
      ],
      "metadata": {
        "id": "nKLxEC8AoYr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This\n",
        " masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
        " predictions for position i can depend only on the known outputs at positions less than i."
      ],
      "metadata": {
        "id": "c_BJUtzKH-7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "네?\n",
        "이거 랜덤입니까? True, False로 이루어진?"
      ],
      "metadata": {
        "id": "Rbo1l1BXH_pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Masking(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        \"\"\"\n",
        "        x.shape = [batch_size, seq_len, data_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        return mask"
      ],
      "metadata": {
        "id": "iIIigiv1od-J"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Layer Normalization(수정 필요)**\n",
        "```python\n",
        "norm = LayerNormalization(d_model, eps)\n",
        "normed = norm(x)\n",
        "```"
      ],
      "metadata": {
        "id": "lOo3enKnofCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not use torch.nn.LayerNorm\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, d_model=16, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        self.d_model = d_model\n",
        "        self.eps = eps # epsilon 이겠지?\n",
        "        self.gamma = nn.Parameter(torch.ones(self.d_model)) # torch document 는 1로 initialize...\n",
        "        self.beta = nn.Parameter(torch.zeros(self.d_model)) # torch document 는 0으로 initialize...\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        temp = x-torch.mean(x)\n",
        "        temp = temp/(torch.sqrt(torch.var(x, unbiased=False) + self.eps))\n",
        "        normed = temp*self.gamma + self.beta # gamma, beta : learnable parameter... 라고 써있는데요?\n",
        "\n",
        "        return normed"
      ],
      "metadata": {
        "id": "UK-sk73UorDq"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reference**\n"
      ],
      "metadata": {
        "id": "3z1VLYEt5pXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Pytorch LayerNorm Document](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)"
      ],
      "metadata": {
        "id": "yLvSxhSw5tsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Layerwise Encoder&Decoder**"
      ],
      "metadata": {
        "id": "CAL6MuaJor6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "nR0X5r6xYT_4"
      },
      "outputs": [],
      "source": [
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a single encoder block consists of the following\n",
        "# multi-head attention, positionwise feed forward network, residual connections, layer normalizations\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,d_model=16,num_head=4,d_ff=32, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.multiattention = MultiHeadAttention(d_model, num_head) # multi-head attention\n",
        "        self.feedforward = PositionwiseFeedForwardNetwork(d_model, d_ff) # positionwise feed forward network\n",
        "        self.dropout = nn.Dropout(drop_prob) # residual Dropout\n",
        "\n",
        "        # layer normalizations\n",
        "        self.norm1 = LayerNormalization(d_model)\n",
        "        self.norm2 = LayerNormalization(d_model)\n",
        "\n",
        "    def forward(self,enc):\n",
        "        # 2-sub layers : Multi Head Attention+Add&Norm, Feed Forward+Add&Norm\n",
        "        residual = enc\n",
        "\n",
        "        output = self.multiattention(enc, enc, enc) # Q=x, K=x, V=x\n",
        "        output = self.norm1(self.dropout(output) + enc)  # Add & Norm\n",
        "\n",
        "        residual = output\n",
        "\n",
        "        output = self.feedforward(output)\n",
        "        output = self.norm2(self.dropout(output)+residual) # Add & Norm\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a single decoder block consists of the following\n",
        "# masked multi-head attention, multi-head attention, positionwise feed forward network, residual connections, layer normalizations\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model=16,num_head=4,d_ff=32,drop_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multiattention1 = MultiHeadAttention(d_model, num_head) # masked\n",
        "        self.multiattention2 = MultiHeadAttention(d_model, num_head)\n",
        "\n",
        "        self.feedforward = PositionwiseFeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = LayerNormalization(d_model)\n",
        "        self.norm2 = LayerNormalization(d_model)\n",
        "        self.norm3 = LayerNormalization(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self,enc_output,dec,dec_mask):\n",
        "        # 3-sub layers\n",
        "        residual = dec\n",
        "\n",
        "        output = self.multiattention1(dec, dec, dec, dec_mask)\n",
        "        output = self.norm1(self.dropout(output)+residual) # Add & Norm\n",
        "\n",
        "        residual = output\n",
        "        # enc_output -> q, k. temp->v\n",
        "        output = self.multiattention2(enc_output, enc_output, output, mask=None)\n",
        "        output = self.norm2(self.dropout(output)+residual) # Add & Norm\n",
        "\n",
        "        residual = output\n",
        "\n",
        "        output = self.feedforward(output) # Feed Forward\n",
        "        output = self.norm3(self.dropout(output)+residual) # Add & Norm\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "NOqwZa21pCYT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **찐 Encoder&Decoder**"
      ],
      "metadata": {
        "id": "0qiUjePQpJud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "in this homework, encoder inputs are not tokens, it is already embeddings in the input dimension\n",
        "hence, you don't have to set input embedding layer\n",
        "instead, you have to transform the input into the hidden dimension with single linear transformation\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "hFbmJ0ZL9pGv",
        "outputId": "bc3af409-d2a5-44e5-dfe3-6f7c8f730e60"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nin this homework, encoder inputs are not tokens, it is already embeddings in the input dimension\\nhence, you don't have to set input embedding layer\\ninstead, you have to transform the input into the hidden dimension with single linear transformation\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a whole encoder, i.e., the left side of Figure 1, consists of the following as well\n",
        "# input embedding, positional encoding\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim # 어디다 써야함\n",
        "        self.num_layer = num_layer # N\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(device, max_len, d_model)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.encoder_N = nn.ModuleList([EncoderLayer(d_model, num_head, d_ff, drop_prob) for _ in range(num_layer)])\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        input_embedding = x # input into the hidden dimension with single linear transformation\n",
        "        pos = self.pos_encoding(input_embedding)\n",
        "\n",
        "        hidden = self.dropout(input_embedding + pos)\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "          hidden = self.encoder_N(hidden)\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "H83Yd4oEo_KN"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a whole decoder, i.e., the left side of Figure 1, consists of the following as well\n",
        "# input embedding, positional encoding, linear classifier\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.num_layer = num_layer # N\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(self.device, max_len,d_model)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.decoder_N = nn.ModuleList([DecoderLayer(d_model, num_head, d_ff, drop_prob) for _ in range(self.num_layer)])\n",
        "\n",
        "    def forward(self,enc_output,y,y_mask):\n",
        "        # fill out here\n",
        "        output_embedding = y\n",
        "        pos_embedding = self.self.pos_encoding(y)\n",
        "\n",
        "        output = self.dropout(output_embedding + pos_embedding)\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "          output = self.decoder_N(enc_output, output, y_mask)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2dZvDTuZ54Xe"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformer**"
      ],
      "metadata": {
        "id": "827gYspjo1zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# sum up encoder and decoder\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, device, input_dim=3, num_layer=3, max_len=512, d_model=16, num_head=4, d_ff=32, drop_prob=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.masking = Masking(self.device)\n",
        "\n",
        "        self.encoder = Encoder(self.device,input_dim,num_layer,max_len,d_model,num_head,d_ff,drop_prob)\n",
        "        self.decoder = Decoder(self.device,input_dim,num_layer,max_len,d_model,num_head,d_ff,drop_prob)\n",
        "\n",
        "        self.linear = nn.Linear(d_model,100)\n",
        "        self.softmax =  nn.Softmax()\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        y_mask = self.masking(x)\n",
        "\n",
        "        enc_output = self.encoder(x)\n",
        "        dec_output = self.decoder(enc_output, y, y_mask)\n",
        "\n",
        "        dec_output = self.linear(dec_output)\n",
        "        dec_output = self.softmax(dec_output)\n",
        "\n",
        "        return dec_output"
      ],
      "metadata": {
        "id": "teeMpId45bpz"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Train Setting**"
      ],
      "metadata": {
        "id": "woBgB4KTlg6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "#### there is nothing to do from here ####\n",
        "##########################################"
      ],
      "metadata": {
        "id": "tJKQKOr2bTx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptimizer:\n",
        "    def __init__(self,optimizer,d_model=16,warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def update_parameter_and_learning_rate(self):\n",
        "        self.optimizer.step()\n",
        "        self.step_num += 1\n",
        "        self.lr = self.d_model**(-.5) * min(self.step_num**(-.5),self.step_num*self.warmup_steps**(-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.lr"
      ],
      "metadata": {
        "id": "xDW3t0IAbSbT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting**"
      ],
      "metadata": {
        "id": "RvUNtrUynlSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# modify num_layer, d_model, num_head, d_ff while debugging your code\n",
        "model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1).to(device)\n",
        "loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
        "scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)"
      ],
      "metadata": {
        "id": "_3fDkmxrnoCS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch 설정\n",
        "num_epoch = 1 # 돌아가나 확인 중..."
      ],
      "metadata": {
        "id": "cJzSmEysmGaU"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train Model**"
      ],
      "metadata": {
        "id": "7DAaQSncl6v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"num_param:\", total_params)"
      ],
      "metadata": {
        "id": "koK2XayWmCuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0275a3ff-c883-4391-9d0d-881b0b7635da"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_param: 18404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train/Evaluate 함수(일단 모델 돌아가면 수정)**"
      ],
      "metadata": {
        "id": "Vwa9EGy-mqyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "반복문 길어서 보기 힘들면 함수로 따로 빼기"
      ],
      "metadata": {
        "id": "X-zkagTmnKsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def train():\n",
        "# train\n",
        "    model.train()\n",
        "\n",
        "    # initialize(epoch마다)\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "        y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "        logit = model.forward(x,y_)\n",
        "        cost = loss(logit, y)\n",
        "\n",
        "        total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss, scheduled_optimizer.lr))\n",
        "'''"
      ],
      "metadata": {
        "id": "GlrO56fimyV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def evaluate():\n",
        "  ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)\n",
        "\n",
        "            total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f\"%(i,ave_loss))\n",
        "'''"
      ],
      "metadata": {
        "id": "aDjI1LbWnMy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for i in range(num_epoch):\n",
        "  train()\n",
        "  evaluate()\n",
        "'''"
      ],
      "metadata": {
        "id": "EW2QGFRznUSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **학습 결과**"
      ],
      "metadata": {
        "id": "V-tjSzjBmve3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epoch):\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    # initialize(epoch마다)\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "        y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "        logit = model.forward(x,y_)\n",
        "        cost = loss(logit, y)\n",
        "\n",
        "        total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss, scheduled_optimizer.lr))\n",
        "\n",
        "    ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            y_ = torch.zeros([y.shape[0],1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)\n",
        "\n",
        "            total_loss += cost.item() * y.shape[0] * y.shape[1] * y.shape[2]\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f\"%(i,ave_loss))"
      ],
      "metadata": {
        "id": "kOBuZW2nlwBu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "6bdda544-96c0-40f7-8fcb-65492f19d9a3"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mask' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-553adc2298e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-7e3f6241971f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-627304e30f95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
          ]
        }
      ]
    }
  ]
}